{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUiuE5j4xkJV"
      },
      "source": [
        "# **AI_Agents_Course Module_1**\n",
        "\n",
        "[Link of the Course](https:///www.coursera.org/learn/ai-agents-python)\n",
        "\n",
        "**Prepared by: Houshyar Jafari Asl**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOO5ziTUK60k"
      },
      "source": [
        "# Sending Prompts Programmatically & Managing Memory 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ym1zXORx4yJ"
      },
      "source": [
        "\"\"\"\n",
        "OLLAMA LLM IN COLAB (LOCAL ALTERNATIVE TO OPENAI)\n",
        "\n",
        "This code sets up a free, local LLM (Llama3) in Google Colab using:\n",
        "1. Ollama - Runs the model locally\n",
        "2. LiteLLM - Provides OpenAI-like API interface\n",
        "\n",
        "HOW IT WORKS:\n",
        "1. First run: Downloads Llama3 (4.7GB, one-time)\n",
        "2. Starts Ollama server with CPU\n",
        "3. Uses LiteLLM to send/receive messages like OpenAI API\n",
        "\n",
        "ADVANTAGES:\n",
        "- No API keys needed\n",
        "- Free to use\n",
        "- Works offline after setup\n",
        "\n",
        "NOTE:\n",
        "- Colab may disconnect after ~1 hour\n",
        "- Responses slightly slower than GPT-4\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8gRqo8jwtVa",
        "outputId": "da2172da-adbf-4d58-d437-4037796d95b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: litellm in /usr/local/lib/python3.11/dist-packages (1.74.9.post1)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.11/dist-packages (from litellm) (3.12.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm) (8.2.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (4.25.0)\n",
            "Requirement already satisfied: openai>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from litellm) (1.97.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (2.11.7)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (1.1.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm) (0.9.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm) (0.21.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10->litellm) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->litellm) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (0.26.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm) (4.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm) (0.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.5.0)\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "What a delightful problem!\n",
            "\n",
            "In functional programming, I'd love to use immutable data structures and recursion (or map/fold) to solve this problem. Here's my solution:\n",
            "```python\n",
            "def swap_dict(d):\n",
            "    return dict(map(lambda x: (x[1], x[0]), d.items()))\n",
            "```\n",
            "Let me break it down:\n",
            "\n",
            "* `d.items()` returns an iterator over the dictionary's key-value pairs as tuples `(key, value)`.\n",
            "* `map` applies a function to each element of the iterator. In this case, I'm using a lambda function that takes a tuple `(key, value)` and returns a new tuple with the values swapped.\n",
            "* The resulting list of tuples is then passed to the `dict` constructor to create a new dictionary with the keys and values swapped.\n",
            "\n",
            "Example usage:\n",
            "```python\n",
            "d = {'a': 1, 'b': 2, 'c': 3}\n",
            "new_d = swap_dict(d)\n",
            "print(new_d)  # Output: {1: 'a', 2: 'b', 3: 'c'}\n",
            "```\n",
            "I hope you enjoy this functional solution!\n"
          ]
        }
      ],
      "source": [
        "!pip install litellm\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start Ollama with GPU support (in background)\n",
        "!OLLAMA_CUDA_OVERRIDE=\"1\" nohup ollama serve > /dev/null 2>&1 &\n",
        "!ollama pull llama3  # Download model (only needed first time)\n",
        "\n",
        "import time\n",
        "time.sleep(10)  # Wait for server to start\n",
        "\n",
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"ollama/llama3\",  # Changed from GPT-4 to Llama3\n",
        "        messages=messages,\n",
        "        max_tokens=1024,\n",
        "        api_base=\"http://localhost:11434\"  # Local Ollama endpoint\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Same prompt as original\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95IGZGKOKnTU"
      },
      "source": [
        "To get started building agents, we need to understand how to send prompts to LLMs. Agents require two key capabilities:\n",
        "\n",
        "1. Programmatic prompting - Automating the prompt-response cycle that humans do manually in a conversation. This forms the foundation of the Agent Loop we’ll explore.\n",
        "\n",
        "2. Memory management - Controlling what information persists between iterations, like API calls and their results, to maintain context through the agent’s decision-making process.\n",
        "\n",
        "Programmatically sending prompts is how we move from having a human type in prompts and then take action based on the LLM’s response to having an agent that can do this automatically. The Agent Loop that we will begin building over the next several readings will be programmatically sending prompts to the LLM and then taking action based on the LLM’s response.\n",
        "\n",
        "We will also need to understand how to manage what the LLM knows or remembers. This is important because we want to be able to control what information the LLM has in each iteration of the loop. For example, if it just called an API, we want it to remember what API it asked to be invoked and what the result of that action was."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7zHFObSKj79"
      },
      "source": [
        "# Sending Prompts Programmatically & Managing Memory 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS1UuCytLN2-"
      },
      "source": [
        "First, let’s take a look at the code again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g3BCl2NLOm_"
      },
      "outputs": [],
      "source": [
        "# !pip install litellm\n",
        "# !curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start Ollama with GPU support (in background)\n",
        "# !OLLAMA_CUDA_OVERRIDE=\"1\" nohup ollama serve > /dev/null 2>&1 &\n",
        "# !ollama pull llama3  # Download model (only needed first time)\n",
        "\n",
        "# import time\n",
        "# time.sleep(10)  # Wait for server to start\n",
        "\n",
        "# from litellm import completion\n",
        "# from typing import List, Dict\n",
        "\n",
        "# def generate_response(messages: List[Dict]) -> str:\n",
        "#     \"\"\"Call LLM to get response\"\"\"\n",
        "#     response = completion(\n",
        "#         model=\"ollama/llama3\",  # Changed from GPT-4 to Llama3\n",
        "#         messages=messages,\n",
        "#         max_tokens=1024,\n",
        "#         api_base=\"http://localhost:11434\"  # Local Ollama endpoint\n",
        "#     )\n",
        "#     return response.choices[0].message.content\n",
        "\n",
        "# Same prompt as original\n",
        "# messages = [\n",
        "#     {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "#     {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "# ]\n",
        "\n",
        "# response = generate_response(messages)\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG-mGB07LRXX"
      },
      "source": [
        "Let’s break down the key components:\n",
        "\n",
        "1. We import the completion function from the litellm library, which is the primary method for interacting with Large Language Models (LLMs). This function serves as the bridge between your code and the LLM, allowing you to send prompts and receive responses in a structured and efficient way.\n",
        "\n",
        "How completion Works:\n",
        "\n",
        "* Input: You provide a prompt, which is a list of messages that you want the model to process. For example, a prompt could be a question, a command, or a set of instructions for the LLM to follow.\n",
        "* Output: The completion function returns the model’s response, typically in the form of generated text based on your prompt.\n",
        "2. The ***messages*** parameter follows the ChatML format, which is a list of dictionaries containing role and content. The role attribute indicates who is “speaking” in the conversation. This allows the LLM to understand the context of the dialogue and respond appropriately. The roles include:\n",
        "\n",
        "* “system”: Provides the model with initial instructions, rules, or configuration for how it should behave throughout the session. This message is not part of the “conversation” but sets the ground rules or context (e.g., “You will respond in JSON.”).\n",
        "* “user”: Represents input from the user. This is where you provide your prompts, questions, or instructions.\n",
        "* “assistant”: Represents responses from the AI model. You can include this role to provide context for a conversation that has already started or to guide the model by showing sample responses. These messages are interpreted as what the “model” said in the past.\n",
        "3. We specify the model using the provider/model format (e.g., “openai/gpt-4o”)\n",
        "\n",
        "4. The response contains the generated text in ***choices[0].message.content***. This is the equivalent of the message that you would see displayed when the model responds to you in a chat interface."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBblgbEOL2xi"
      },
      "source": [
        "# Sending Prompts Programmatically & Managing Memory 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Llk4ii9L5ow"
      },
      "source": [
        "System messages are particularly important in the conversation and will be very important for AI agents. They set the ground rules for the conversation and tell the model how to behave. Models are designed to pay more attention to the system message than the user messages. We can “program” the AI agent through system messages.\n",
        "\n",
        "Let’s simulate a customer service interaction for a customer service agent that always tells the customer to turn off their computer or modem with system messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqvqq99iL0RY",
        "outputId": "d1d7e475-5662-4bbe-ffe4-7abefb81e5f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm happy to help you with that! It sounds like your internet might be experiencing some connectivity issues. Don't worry, it's a super common problem!\n",
            "\n",
            "Before we dive into more complex troubleshooting steps, have you tried the old but trusty \"turn it off and on\" solution? Yeah, I know, it sounds simple, but sometimes it really is that easy! Just shut down your computer or modem (whichever one is responsible for your internet connection), wait for about 10-15 seconds, and then turn it back on. Give it a minute to boot up and reconnect.\n",
            "\n",
            "In most cases, this simple reboot can resolve the issue and get your internet up and running smoothly again. So, would you like to give that a try?\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How do I get my Internet working again.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsAHaumdMPP_"
      },
      "source": [
        "The system message is the most important part of this prompt. It tells the model how to behave. The user message is the question that we want the model to answer. The system instructions lay the ground rules for the interaction.\n",
        "\n",
        "The messages can incorporate arbitrary information as long as it is in text form. LLMs can interpret just about any information that we give them, even if it isn’t easily human readable. Let’s generate an implementation of a function based on some information in a dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJf92ZhLMRju",
        "outputId": "91899a29-89d9-45b6-a116-65047b13bbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the implementation of the `swap_keys_values` function:\n",
            "```\n",
            "def swap_keys_values(d):\n",
            "    \"\"\"\n",
            "    Swaps the keys and values in a given dictionary.\n",
            "\n",
            "    Args:\n",
            "        d (dict): A dictionary with unique values.\n",
            "\n",
            "    Returns:\n",
            "        dict: The original dictionary with its keys and values swapped.\n",
            "    \"\"\"\n",
            "    return {v: k for k, v in d.items()}\n",
            "```\n",
            "Here's an explanation of how I implemented this function:\n",
            "\n",
            "* I used a dictionary comprehension to create a new dictionary where the keys are the original values and vice versa.\n",
            "* I used the `.items()` method to iterate over the key-value pairs of the input dictionary `d`.\n",
            "* For each pair, I swapped the key and value using `{v: k for k, v in d.items()}`.\n",
            "\n",
            "Note that this implementation assumes that the input dictionary has unique values. If the input dictionary has duplicate values, this function will raise a `ValueError` because dictionaries cannot have duplicate keys.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "code_spec = {\n",
        "    'name': 'swap_keys_values',\n",
        "    'description': 'Swaps the keys and values in a given dictionary.',\n",
        "    'params': {\n",
        "        'd': 'A dictionary with unique values.'\n",
        "    },\n",
        "}\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "     \"content\": \"You are an expert software engineer that writes clean functional code. You always document your functions.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Please implement: {json.dumps(code_spec)}\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olTt4VxzMUB_"
      },
      "source": [
        "We will rely heavily on the ability to send the LLM just about any type of information, particularly JSON, when we start building agents. This is a simple example of how we can use JSON to send information to the LLM, but you can see how we could provide it JSON with information about the result of an API call, for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbXDOf4QMvll"
      },
      "source": [
        "# Giving Agents Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai2vzq3SMxNc"
      },
      "source": [
        "When we are building an Agent, we need it to remember its actions and the result of those actions. For example, if it tries to create a calendar event for a meeting and the API call fails due to an incorrect parameter value that it provided, we want it to remember that the API call failed and why. This way, it can correct the mistake and try again. If we have a complex task that we break down into multiple steps, we need the Agent to remember the results of each step to ensure that it can continue the task from where it left off. Memory is crucial for Agents.\n",
        "\n",
        "**LLMs Do Not Have Memory**\n",
        "\n",
        "When interacting with an LLM, **the model does not inherently “remember” previous conversations or responses**. Every time you call the model, it generates a response based solely on the information provided in the messages parameter. If previous context is not included in the messages, the model will not have any knowledge of it.\n",
        "\n",
        "This means that to simulate continuity in a conversation, you must explicitly pass all relevant prior messages (including system, user, and assistant roles) in the messages list for each request.\n",
        "\n",
        "Example 1: Missing Context in the Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8rwG8H_M7iS",
        "outputId": "a133ca0a-9d32-47dc-fc47-6e10a304d3e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Response\n",
            "A delightful problem!\n",
            "\n",
            "In functional programming, I'd recommend using recursion and immutable data structures to achieve this. Here's a possible solution:\n",
            "\n",
            "```haskell\n",
            "swapKV :: Ord k => [(k, v)] -> [(v, k)]\n",
            "swapKV = map (\\(k, v) -> (v, k))\n",
            "```\n",
            "\n",
            "Let me break it down:\n",
            "\n",
            "* `Ord k` is a type constraint that ensures the keys are orderable. This is necessary because we're using recursion and need to compare keys.\n",
            "* `[(k, v)]` represents the input dictionary as a list of key-value pairs.\n",
            "* `(v, k)` is the output pair with values and keys swapped.\n",
            "* `map` applies a transformation function to each element in the list.\n",
            "\n",
            "This implementation is functional, pure, and composable. It doesn't modify the original dictionary and returns a new one with the desired swap.\n",
            "\n",
            "If you prefer an imperative approach or want to use a specific language other than Haskell, I can provide alternative solutions. Just let me know!\n",
            "Second Response\n",
            "I'd be happy to help! However, I need more context about what function you would like me to update with documentation. Could you please provide the function and its purpose?\n",
            "\n",
            "Once I have that information, I can assist you in adding helpful comments and docstrings to make your code easier to understand and maintain.\n",
            "\n",
            "For example, if we were working on a simple calculator program, here's an updated version of a `add` function with documentation:\n",
            "\n",
            "```\n",
            "def add(x: int, y: int) -> int:\n",
            "    \"\"\"\n",
            "    This function adds two integers together.\n",
            "\n",
            "    Args:\n",
            "        x (int): The first integer to be added.\n",
            "        y (int): The second integer to be added.\n",
            "\n",
            "    Returns:\n",
            "        int: The sum of the two input integers.\n",
            "    \"\"\"\n",
            "    return x + y\n",
            "```\n",
            "\n",
            "Please provide me with your function, and I'll help you add documentation that accurately describes its purpose, parameters, and return values.\n"
          ]
        }
      ],
      "source": [
        "# First query\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(\"First Response\")\n",
        "print(response)\n",
        "\n",
        "# Second query without including the previous response\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Update the function to include documentation.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(\"Second Response\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idm6yTrxNOaR"
      },
      "source": [
        "**Explanation:** In the second request, the model doesn’t “remember” the function it wrote in the first interaction. Since the information is not included in the second prompt, the model cannot connect the two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4tJPFNjNRiI"
      },
      "source": [
        "**Example 2: Including Previous Responses for Continuity**\n",
        "\n",
        "To fix this issue, we need to add new messages with the “assistant” role to the messages list with the content of the prior response from the LLM. This way, the model can see what code it wrote previously and can build on that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQpgvlklNoU3",
        "outputId": "21354339-58ae-4d9f-80d3-e6a0be0aedcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A delightful problem!\n",
            "\n",
            "In functional programming, I'll write a concise and composable solution using Python's built-in `map` function. Here it is:\n",
            "```python\n",
            "def swap_keys_values(d):\n",
            "    return dict(map(lambda x: (x[1], x[0]), d.items()))\n",
            "```\n",
            "Let me explain how it works:\n",
            "\n",
            "* `d.items()` returns an iterator over the dictionary's key-value pairs as tuples, where each tuple contains a key-value pair.\n",
            "* The lambda function takes each tuple `(k, v)` and swaps its elements, effectively transforming it into `(v, k)`.\n",
            "* `map` applies this lambda function to each tuple in the iterator, producing a new iterator of swapped tuples.\n",
            "* Finally, we convert the resulting iterator back into a dictionary using the `dict` constructor.\n",
            "\n",
            "Example usage:\n",
            "```python\n",
            "d = {'a': 1, 'b': 2, 'c': 3}\n",
            "swapped_d = swap_keys_values(d)\n",
            "print(swapped_d)  # Output: {1: 'a', 2: 'b', 3: 'c'}\n",
            "```\n",
            "This solution is not only concise but also functional, as it avoids modifying the original dictionary and produces a new one with swapped keys and values.\n",
            "I'm glad you liked the initial solution! Here's the updated code with documentation:\n",
            "\n",
            "```python\n",
            "def swap_keys_values(d: dict) -> dict:\n",
            "    \"\"\"\n",
            "    Swaps the keys and values in a dictionary.\n",
            "\n",
            "    Args:\n",
            "        d (dict): The input dictionary.\n",
            "\n",
            "    Returns:\n",
            "        dict: A new dictionary with swapped key-value pairs.\n",
            "    \"\"\"\n",
            "    return dict(map(lambda x: (x[1], x[0]), d.items()))\n",
            "```\n",
            "\n",
            "In this documentation, I've specified the function's input type (`d: dict`) and output type (`-> dict`). This helps other developers understand how to use the function correctly.\n",
            "\n",
            "For example, when someone reads the code, they'll see that `swap_keys_values` expects a dictionary as an argument (type hinting is nice for this!) and returns a new dictionary with swapped key-value pairs.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "   {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "   {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "\n",
        "# We are going to make this verbose so it is clear what\n",
        "# is going on. In a real application, you would likely\n",
        "# just append to the messages list.\n",
        "messages = [\n",
        "   {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "   {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"},\n",
        "\n",
        "   # Here is the assistant's response from the previous step\n",
        "   # with the code. This gives it \"memory\" of the previous\n",
        "   # interaction.\n",
        "   {\"role\": \"assistant\", \"content\": response},\n",
        "\n",
        "   # Now, we can ask the assistant to update the function\n",
        "   {\"role\": \"user\", \"content\": \"Update the function to include documentation.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTAfCjZJPReA"
      },
      "source": [
        "Proper Conversation Flow (Without Repetition and Appending Added)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4Zo-td0IPTv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0b0d1f-2d40-470a-e5cc-7812a4e4e616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIRST RESPONSE:\n",
            "What a delightful problem! In functional programming, I'd approach this by using a combination of `map` and `invert` (or `zip`) functions. Here's my solution:\n",
            "\n",
            "```python\n",
            "def swap_keys_and_values(d):\n",
            "    return dict(zip(d.values(), d.keys()))\n",
            "```\n",
            "\n",
            "Let me explain what's going on here:\n",
            "\n",
            "1. `d.values()` returns an iterator over the values in the dictionary.\n",
            "2. `d.keys()` returns an iterator over the keys in the dictionary.\n",
            "3. The `zip` function pairs up these two iterators, effectively \"swapping\" the keys and values.\n",
            "4. Finally, we use the `dict` constructor to create a new dictionary from this paired-up data.\n",
            "\n",
            "This solution is not only concise but also efficient, as it avoids creating intermediate lists or other unnecessary data structures. It's a beautiful example of functional programming in action!\n",
            "\n",
            "Example usage:\n",
            "```python\n",
            "d = {'a': 1, 'b': 2, 'c': 3}\n",
            "result = swap_keys_and_values(d)\n",
            "print(result)  # Output: {1: 'a', 2: 'b', 3: 'c'}\n",
            "```\n",
            "\n",
            "I hope you enjoy this solution!\n",
            "\n",
            "IMPROVED VERSION:\n",
            "Here is the updated code with documentation and type hints:\n",
            "\n",
            "```python\n",
            "def swap_keys_and_values(d: dict) -> dict:\n",
            "    \"\"\"\n",
            "    Swap the keys and values in a dictionary.\n",
            "\n",
            "    This function takes a dictionary as input, swaps its keys and values using the `zip`\n",
            "    function, and returns the resulting dictionary. Note that this operation is not\n",
            "    necessarily invertible, since dictionaries can have duplicate values or keys.\n",
            "\n",
            "    Args:\n",
            "        d: The input dictionary to be swapped.\n",
            "\n",
            "    Returns:\n",
            "        A new dictionary with the keys and values of the original dictionary swapped.\n",
            "    \"\"\"\n",
            "    return dict(zip(d.values(), d.keys()))\n",
            "```\n",
            "\n",
            "I've added a docstring that explains what the function does, as well as its arguments and return value. I've also added type hints for the input `d` (which should be a dictionary) and the returned value (also a dictionary).\n",
            "\n",
            "Example usage remains the same:\n",
            "\n",
            "```python\n",
            "d = {'a': 1, 'b': 2, 'c': 3}\n",
            "result = swap_keys_and_values(d)\n",
            "print(result)  # Output: {1: 'a', 2: 'b', 3: 'c'}\n",
            "```\n",
            "\n",
            "Now, both the code and its documentation are self-documenting, making it easier for others (and yourself!) to understand how to use this function.\n"
          ]
        }
      ],
      "source": [
        "# Initial request\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "# First response\n",
        "first_response = generate_response(messages)\n",
        "print(\"FIRST RESPONSE:\")\n",
        "print(first_response)\n",
        "\n",
        "# Follow-up request (appends naturally to conversation)\n",
        "messages.append({\n",
        "    \"role\": \"assistant\",\n",
        "    \"content\": first_response  # Maintains memory\n",
        "})\n",
        "messages.append({\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"Please add documentation and type hints.\"  # Only new request\n",
        "})\n",
        "\n",
        "# Second response\n",
        "second_response = generate_response(messages)\n",
        "print(\"\\nIMPROVED VERSION:\")\n",
        "print(second_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gct-nM3YQcpv"
      },
      "source": [
        "**Key Takeaways**\n",
        "\n",
        "1. **No Inherent Memory**: The LLM has no knowledge of past interactions unless explicitly provided in the current prompt (via messages).\n",
        "2. **Provide Full Context**: To simulate continuity in a conversation, include all relevant messages (both user and assistant responses) in the messages parameter.\n",
        "3. **Role of Assistant Messages**: Adding previous responses as assistant messages allows the model to maintain a coherent conversation and build on earlier exchanges. For an agent, this will allow it to remember what actions, such as API calls, it took in the past.\n",
        "4. **Memory Management**: We can control what the LLM remembers or does not remember by managing what messages go into the conversation. Causing the LLM to forget things can be a powerful tool in some circumstances, such as when we need to break a pattern of poor responses from an Agent.\n",
        "\n",
        "**Why This Matters**\n",
        "Understanding the stateless nature of LLMs is crucial for designing agents that rely on multi-turn conversations with their environment. Developers must explicitly manage and provide context to ensure the model generates accurate and relevant responses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob0hu6x3Q_0M"
      },
      "source": [
        "# Building a Quasi-Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfuXkpMyRB4H"
      },
      "source": [
        "For practice, we are going to write a quasi-agent that can write Python functions based on user requirements. It isn’t quite a real agent, it can’t react and adapt, but it can do something useful for us.\n",
        "\n",
        "The quasi-agent will ask the user what they want code for, write the code for the function, add documentation, and finally include test cases using the unittest framework. This exercise will help you understand how to maintain context across multiple prompts and manage the information flow between the user and the LLM. It will also help you understand the pain of trying to parse and handle the output of an LLM that is not always consistent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctnGBm4FRDm5"
      },
      "source": [
        "Practice Exercise\n",
        "\n",
        "This exercise will allow you to practice programmatically sending prompts to an LLM and managing memory.\n",
        "\n",
        "For this exercise, you should write a program that uses sequential prompts to generate any Python function based on user input. The program should:\n",
        "\n",
        "1. First Prompt:\n",
        "\n",
        "* Ask the user what function they want to create\n",
        "* Ask the LLM to write a basic Python function based on the user’s description\n",
        "* Store the response for use in subsequent prompts\n",
        "* Parse the response to separate the code from the commentary by the LLM\n",
        "\n",
        "2. Second Prompt:\n",
        "* Pass the code generated from the first prompt\n",
        "* Ask the LLM to add comprehensive documentation including:\n",
        "* Function description\n",
        "* Parameter descriptions\n",
        "* Return value description\n",
        "* Example usage\n",
        "* Edge cases\n",
        "\n",
        "3. Third Prompt:\n",
        "* Pass the documented code generated from the second prompt\n",
        "* Ask the LLM to add test cases using Python’s unittest framework\n",
        "* Tests should cover:\n",
        "* Basic functionality\n",
        "* Edge cases\n",
        "* Error cases\n",
        "* Various input scenarios"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "import time\n",
        "import re\n",
        "import sys\n",
        "from typing import List, Dict\n",
        "\n",
        "def generate_response(messages: List[Dict], retries=3) -> str:\n",
        "    \"\"\"Enhanced with automatic retries\"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = completion(\n",
        "                model=\"ollama/llama3\",\n",
        "                messages=messages,\n",
        "                api_base=\"http://localhost:11434\",\n",
        "                max_tokens=1024,\n",
        "                timeout=120,  # Extended timeout\n",
        "                temperature=0.7\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt+1} failed: {str(e)}\")\n",
        "            if attempt < retries - 1:\n",
        "                print(\"Waiting and retrying...\")\n",
        "                time.sleep(15)  # Shorter wait between retries\n",
        "            continue\n",
        "    print(\"\\n❌ All retries failed. Check server with:\")\n",
        "    print(\"!ps aux | grep ollama\")\n",
        "    print(\"!cat server.log\")\n",
        "    sys.exit(1)\n",
        "\n",
        "def extract_code_block(response: str) -> str:\n",
        "    \"\"\"Robust code extraction\"\"\"\n",
        "    patterns = [\n",
        "        r'```python\\n(.*?)\\n```',  # Standard markdown\n",
        "        r'```\\n(.*?)\\n```',        # No language specified\n",
        "        r'def .*?\\):.*?\\n(.*?)(?=\\n\\S|\\Z)'  # Raw code detection\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, response, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "    return response  # Fallback to full response\n",
        "\n",
        "def develop_custom_function():\n",
        "    print(\"\\nWhat kind of function would you like to create?\")\n",
        "    print(\"Example: 'A function that calculates factorial'\")\n",
        "    print(\"Your description: \", end='')\n",
        "    function_description = input().strip()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a Python expert. Return ONLY code in markdown blocks.\"}\n",
        "    ]\n",
        "\n",
        "    # First prompt - Basic function\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Write a Python function that {function_description}. Return ONLY the code in a ```python block.\"\n",
        "    })\n",
        "\n",
        "    print(\"\\n🧠 Generating initial function...\")\n",
        "    initial_response = generate_response(messages)\n",
        "    initial_code = extract_code_block(initial_response)\n",
        "\n",
        "    print(\"\\n=== Initial Function ===\")\n",
        "    print(initial_code)\n",
        "\n",
        "    # Second prompt - Documentation\n",
        "    messages.append({\"role\": \"assistant\", \"content\": f\"```python\\n{initial_code}\\n```\"})\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Add documentation including:\\n\"\n",
        "                   \"- Function description\\n\"\n",
        "                   \"- Parameter types\\n\"\n",
        "                   \"- Return type\\n\"\n",
        "                   \"- Example usage\\n\"\n",
        "                   \"- Edge cases\\n\"\n",
        "                   \"Return ONLY the documented code in a ```python block.\"\n",
        "    })\n",
        "\n",
        "    print(\"\\n📝 Adding documentation...\")\n",
        "    doc_response = generate_response(messages)\n",
        "    documented_code = extract_code_block(doc_response)\n",
        "\n",
        "    print(\"\\n=== Documented Function ===\")\n",
        "    print(documented_code)\n",
        "\n",
        "    # Third prompt - Tests\n",
        "    messages.append({\"role\": \"assistant\", \"content\": f\"```python\\n{documented_code}\\n```\"})\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Create unittest tests covering:\\n\"\n",
        "                   \"- Basic functionality\\n\"\n",
        "                   \"- Edge cases\\n\"\n",
        "                   \"- Error handling\\n\"\n",
        "                   \"Return ONLY the test code in a ```python block.\"\n",
        "    })\n",
        "\n",
        "    print(\"\\n🧪 Generating tests...\")\n",
        "    test_response = generate_response(messages)\n",
        "    test_code = extract_code_block(test_response)\n",
        "\n",
        "    print(\"\\n=== Test Cases ===\")\n",
        "    print(test_code)\n",
        "\n",
        "    # Save to file\n",
        "    filename = function_description[:30].replace(' ', '_') + '.py'\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write(f\"# Implementation\\n{documented_code}\\n\\n# Tests\\n{test_code}\")\n",
        "\n",
        "    return documented_code, test_code, filename\n",
        "\n",
        "# Quick server health check\n",
        "try:\n",
        "    import requests\n",
        "    requests.get(\"http://localhost:11434\", timeout=10)\n",
        "    print(\"✅ Ollama server is ready\")\n",
        "except:\n",
        "    print(\"⚠️ Server not responding - run initialization cells first!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Run the function generator\n",
        "function, tests, filename = develop_custom_function()\n",
        "print(f\"\\n✅ Success! Saved to {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjKTJ8KrZtZ5",
        "outputId": "2cf67ccf-a0d1-4091-8277-f9c39770178a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Ollama server is ready\n",
            "\n",
            "What kind of function would you like to create?\n",
            "Example: 'A function that calculates factorial'\n",
            "Your description: A function that calculates factorial\n",
            "\n",
            "🧠 Generating initial function...\n",
            "\n",
            "=== Initial Function ===\n",
            "def calculate_factorial(n):\n",
            "    if n == 0 or n == 1:\n",
            "        return 1\n",
            "    else:\n",
            "        return n * calculate_factorial(n-1)\n",
            "\n",
            "📝 Adding documentation...\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "Attempt 1 failed: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out after 120.0 seconds.\n",
            "Waiting and retrying...\n",
            "\n",
            "=== Documented Function ===\n",
            "def calculate_factorial(n: int) -> int:\n",
            "    \"\"\"\n",
            "    Calculate the factorial of a given integer.\n",
            "\n",
            "    Args:\n",
            "        n (int): The input number for which to calculate the factorial.\n",
            "\n",
            "    Returns:\n",
            "        int: The factorial of the input number.\n",
            "\n",
            "    Example usage:\n",
            "        >>> calculate_factorial(5)\n",
            "        120\n",
            "\n",
            "    Edge cases:\n",
            "        - If n is 0 or 1, return 1 (since the factorial of 0 and 1 is 1).\n",
            "    \"\"\"\n",
            "\n",
            "🧪 Generating tests...\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "Attempt 1 failed: litellm.APIConnectionError: OllamaException - litellm.Timeout: Connection timed out after 120.0 seconds.\n",
            "Waiting and retrying...\n",
            "\n",
            "=== Test Cases ===\n",
            "import unittest\n",
            "\n",
            "class TestCalculateFactorial(unittest.TestCase):\n",
            "\n",
            "    def test_basic_functionality(self):\n",
            "        self.assertEqual(calculate_factorial(5), 120)\n",
            "        self.assertEqual(calculate_factorial(3), 6)\n",
            "\n",
            "    def test_edge_cases(self):\n",
            "        self.assertEqual(calculate_factorial(0), 1)\n",
            "        self.assertEqual(calculate_factorial(1), 1)\n",
            "\n",
            "    def test_error_handling(self):\n",
            "        with self.assertRaises(TypeError):\n",
            "            calculate_factorial('a')\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    unittest.main()\n",
            "\n",
            "✅ Success! Saved to A_function_that_calculates_fac.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple AI Agent, Part 1"
      ],
      "metadata": {
        "id": "PBc8W71ThkBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you understand the agent loop and how to craft effective prompts, we can build a simple AI agent. This agent will be able to list files in a directory, read their content, and answer questions about them. We’ll break down the agent loop—how it receives input, decides on actions, executes them, and updates its memory—step by step.\n",
        "\n",
        "**The Agent Loop in Python**\n",
        "The agent loop is the backbone of our AI agent, enabling it to perform tasks by combining response generation, action execution, and memory updates in an iterative process. This section focuses on how the agent loop works and its role in making the agent dynamic and adaptive.\n",
        "\n",
        "1. **Construct Prompt**: Combine the agent’s memory, user input, and system rules into a single prompt. This ensures the LLM has all the context it needs to decide on the next action, maintaining continuity across iterations.\n",
        "\n",
        "2. **Generate Response**: Send the constructed prompt to the LLM and retrieve a response. This response will guide the agent’s next step by providing instructions in a structured format.\n",
        "\n",
        "3. **Parse Response**: Extract the intended action and its parameters from the LLM’s output. The response must adhere to a predefined structure (e.g., JSON format) to ensure it can be interpreted correctly.\n",
        "\n",
        "4. **Execute Action**: Use the extracted action and its parameters to perform the requested task with the appropriate tool. This could involve listing files, reading content, or printing a message.\n",
        "\n",
        "5. **Convert Result to String**: Format the result of the executed action into a string. This allows the agent to store the result in its memory and provide clear feedback to the user or itself.\n",
        "\n",
        "6. **Continue Loop?**: Evaluate whether the loop should continue based on the current action and results. The loop may terminate if a “terminate” action is specified or if the agent has completed the task.\n",
        "\n",
        "The agent iterates through this loop, refining its behavior and adapting its actions until it reaches a stopping condition. This process is what enables the agent to interact dynamically and respond intelligently to tasks.\n",
        "\n",
        "Here’s how these steps come together in code:"
      ],
      "metadata": {
        "id": "GYJlSlC1hl7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Agent Loop\n",
        "while iterations < max_iterations:\n",
        "\n",
        "    # 1. Construct prompt: Combine agent rules with memory\n",
        "    prompt = agent_rules + memory\n",
        "\n",
        "    # 2. Generate response from LLM\n",
        "    print(\"Agent thinking...\")\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Agent response: {response}\")\n",
        "\n",
        "    # 3. Parse response to determine action\n",
        "    action = parse_action(response)\n",
        "\n",
        "    result = \"Action executed\"\n",
        "\n",
        "    if action[\"tool_name\"] == \"list_files\":\n",
        "        result = {\"result\":list_files()}\n",
        "    elif action[\"tool_name\"] == \"read_file\":\n",
        "        result = {\"result\":read_file(action[\"args\"][\"file_name\"])}\n",
        "    elif action[\"tool_name\"] == \"error\":\n",
        "        result = {\"error\":action[\"args\"][\"message\"]}\n",
        "    elif action[\"tool_name\"] == \"terminate\":\n",
        "        print(action[\"args\"][\"message\"])\n",
        "        break\n",
        "    else:\n",
        "        result = {\"error\":\"Unknown action: \"+action[\"tool_name\"]}\n",
        "\n",
        "    print(f\"Action result: {result}\")\n",
        "\n",
        "    # 5. Update memory with response and results\n",
        "    memory.extend([\n",
        "        {\"role\": \"assistant\", \"content\": response},\n",
        "        {\"role\": \"user\", \"content\": json.dumps(result)}\n",
        "    ])\n",
        "\n",
        "    # 6. Check termination condition\n",
        "    if action[\"tool_name\"] == \"terminate\":\n",
        "        break\n",
        "\n",
        "    iterations += 1"
      ],
      "metadata": {
        "id": "2uLAW02ss8Pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Constructing the Agent Prompt**\n",
        "\n",
        "The prompt is created by appending the agent’s rules (system message) to the current memory of interactions. Part of the memory is a descripton of the task that the agent should perofrm. This ensures the agent is always aware of its tools and constraints while also remembering past actions.\n",
        "\n",
        "`prompt = agent_rules + memory`\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* *agent_rules*: This contains the predefined system instructions, ensuring the agent behaves within its defined constraints and understands its tools.\n",
        "* *memory*: This is a record of all past interactions, including user input, the agent’s responses, and the results of executed actions.\n",
        "\n",
        "By constructing the prompt this way, the agent retains continuity across iterations, ensuring it can adapt its behavior based on previous actions and results. The memory tells it what just happened, what happened in the past, and informs its decision of the next action."
      ],
      "metadata": {
        "id": "jP8Ocd3XxKKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agent Rules: Defining the Agent’s Behavior**\n",
        "\n",
        "Before the agent begins its loop, it must have a clear set of rules that define its behavior, capabilities, and constraints. These agent rules are specified in the system message and play a critical role in ensuring the agent interacts predictably and within its defined boundaries.\n",
        "\n",
        "**How it works in code:**\n",
        "\n",
        "The `agent_rules` are written as a system message that instructs the LLM on how the agent should behave, what tools it has available, and how to format its responses. These rules are included at the start of the prompt for every iteration."
      ],
      "metadata": {
        "id": "4OeWcalrx0bV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_rules = [{\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "You are an AI agent that can perform tasks by using available tools.\n",
        "\n",
        "Available tools:\n",
        "- list_files() -> List[str]: List all files in the current directory.\n",
        "- read_file(file_name: str) -> str: Read the content of a file.\n",
        "- terminate(message: str): End the agent loop and print a summary to the user.\n",
        "\n",
        "If a user asks about files, list them before reading.\n",
        "\n",
        "Every response MUST have an action.\n",
        "Respond in this format:\n",
        "\n",
        "```action\n",
        "{\n",
        "    \"tool_name\": \"insert tool_name\",\n",
        "    \"args\": {...fill in any required arguments here...}\n",
        "}"
      ],
      "metadata": {
        "id": "772Yb1H_xWrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "* **Role of system messages**: The `system` role in the messages list is used to establish ground rules for the agent. This ensures the LLM understands what it can do and how it should behave throughout the session.\n",
        "* **Tools description**: The agent rules explicitly list the tools the agent can use, providing a structured interface for interaction with the environment.\n",
        "* **Output format**: The rules enforce a standardized output format (\"```action {...}\"), which makes parsing and executing actions easier and less error-prone.\n",
        "\n",
        "Each of the “tools” in the system prompt correspond to a function in the code. The agent is going to choose what function to execute and when. Moreover, it is going to decide the parameters that are provided to the functions.\n",
        "\n",
        "The agent is not creating the functions at this point; it is orchestrating their behavior. This means that the logic for how each tool operates is predefined in the code, and the agent focuses on selecting the right tool for the job and providing the correct input to that tool.\n",
        "\n",
        "Because agents can adapt as the loop progresses, they can dynamically decide which tool to use based on the current context and task requirements. This ability allows the agent to adjust its behavior as new information becomes available, making it more flexible and responsive to the user’s input.\n",
        "\n",
        "For example, if the user asks the agent to read the contents of a specific file, the agent will first use the `list_files` tool to identify the available files. Then, based on the result, it will determine whether to proceed with the `read_files` tool or respond with an error if the file does not exist. The agent evaluates each step iteratively, ensuring its actions are informed by the current state of the environment.\n",
        "\n",
        "This orchestration process, driven by the agent rules and the tools available, showcases the power of combining pre-defined functions with adaptive decision-making. By allowing the agent to focus on what to do rather than how to do it, we create a system that leverages the LLM for high-level reasoning while relying on well-defined code for execution.\n",
        "\n",
        "This separation of reasoning and execution is what makes the agent loop so powerful—it creates a modular, extensible framework that can handle increasingly complex tasks without rewriting the underlying tools.\n",
        "\n",
        "Additionally, the agent loop eliminates much of the “glue code” traditionally required to tie these fundamental functions together. Instead of hardcoding workflows, the agent dynamically decides the sequence of actions needed to achieve a task, effectively realizing a program on top of its components. This dynamic nature enables the agent to combine its tools in ways that would typically require custom logic, making it far more versatile and capable of addressing a broader range of use cases without additional development overhead."
      ],
      "metadata": {
        "id": "Dk8H3WjtyFmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example in practice:**\n",
        "\n",
        "If the user asks, “What files are here?”, the agent rules guide the LLM to respond with something like:\n",
        "\n",
        "`{\"tool_name\": \"list_files\", \"args\": {}}`\n",
        "\n",
        "This response ensures the agent’s next step is both predictable and executable within its predefined constraints."
      ],
      "metadata": {
        "id": "xNrT4Vm_yljp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How agent_rules integrate with the loop:**\n",
        "\n",
        "The `agent_rules` are combined with the memory in **Step 1: Construct Prompt** to form the input for the LLM. This guarantees that the agent always has access to its instructions and tools at every iteration. We will discuss the memory in more detail later.\n",
        "\n",
        "This step prepares the input for the LLM by combining the system rules and the memory of the agent’s previous interactions. The goal is to give the LLM all the necessary context for generating the next action.\n",
        "\n",
        "**Example in practice:**\n",
        "\n",
        "If the user asks, “What files are in this directory?”, the memory might look like this:"
      ],
      "metadata": {
        "id": "JOtRNuT0zpkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = [\n",
        "    {\"role\": \"user\", \"content\": \"What files are in this directory?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"```action\\n{\\\"tool_name\\\":\\\"list_files\\\",\\\"args\\\":{}}\\n```\"},\n",
        "    {\"role\": \"user\", \"content\": \"[\\\"file1.txt\\\", \\\"file2.txt\\\"]\"}\n",
        "]"
      ],
      "metadata": {
        "id": "HacMjiQl00Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding `agent_rules` ensures the LLM understands what tools it can use to continue interacting."
      ],
      "metadata": {
        "id": "NipuXYey00s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Generate Response**\n",
        "\n",
        "After constructing the prompt, the agent sends it to the LLM to receive a response. This response will define the next action for the agent to execute.\n",
        "\n",
        "**Code snippet:**\n",
        "\n",
        "`response = generate_response(prompt)`\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "The `generate_response` function uses the LiteLLM library to send the prompt to the LLM and retrieve its response. The response typically includes a structured action that the agent will parse and execute in the next steps. This is where the LLM decides what action the agent should take, based on the provided context and rules."
      ],
      "metadata": {
        "id": "VAnseBPB07k5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple AI Agent, Part 2"
      ],
      "metadata": {
        "id": "DJsAgxjj1HS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the Agent has generated a response, we need to interface the agent with the environment. This involves figuring out how the Agent’s response corresponds to an action in the environment. Once the correct action is determined, the interface can execute the action and later provide the Agent feedback on the result of the action.\n",
        "\n",
        "**Step 3: Parse the Response**\n",
        "\n",
        "After generating a response, the next step is to extract the intended action and its parameters from the LLM’s output. The response is expected to follow a predefined structure, such as a JSON format encapsulated within a markdown code block. This structure ensures the action can be parsed and executed without ambiguity.\n",
        "\n",
        "In the code, this is accomplished by locating and extracting the content between the ```action markers. If the response does not include a valid action block, the agent defaults to a termination action, returning the raw response as the message:"
      ],
      "metadata": {
        "id": "298cZu7o2Sb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_action(response: str) -> Dict:\n",
        "    \"\"\"Parse the LLM response into a structured action dictionary.\"\"\"\n",
        "    try:\n",
        "        response = extract_markdown_block(response, \"action\")\n",
        "        response_json = json.loads(response)\n",
        "        if \"tool_name\" in response_json and \"args\" in response_json:\n",
        "            return response_json\n",
        "        else:\n",
        "            return {\"tool_name\": \"error\", \"args\": {\"message\": \"You must respond with a JSON tool invocation.\"}}\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"tool_name\": \"error\", \"args\": {\"message\": \"Invalid JSON response. You must respond with a JSON tool invocation.\"}}"
      ],
      "metadata": {
        "id": "bAqwheS92xNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This parsing step is critical to ensuring the response is actionable. It provides a structured output, such as:"
      ],
      "metadata": {
        "id": "d96s_NUV2yQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    \"tool_name\": \"list_files\",\n",
        "    \"args\": {}\n",
        "}"
      ],
      "metadata": {
        "id": "y6TG976c2zsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By breaking down the LLM’s output into `tool_name` and `args`, the agent can precisely determine the next action and its inputs.\n",
        "\n",
        "If the LLM response does not contain a valid action block, the agent defaults to an error message, prompting the LLM to provide a valid JSON tool invocation. The error message appears to have come from the “user”. This fallback mechanism ensures the agent can recover if it starts outputting invalid responses that aren’t in the desired format."
      ],
      "metadata": {
        "id": "oj_-Befw21ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Execute the Action**\n",
        "\n",
        "Once the response is parsed, the agent uses the extracted `tool_name` and `args` to execute the corresponding function. Each predefined tool in the system instructions corresponds to a specific function in the code, enabling the agent to interact with its environment.\n",
        "\n",
        "The execution logic involves mapping the `tool_name` to the appropriate function and passing the provided arguments:"
      ],
      "metadata": {
        "id": "XCvekR_v25uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if action[\"tool_name\"] == \"list_files\":\n",
        "    result = {\"result\": list_files()}\n",
        "elif action[\"tool_name\"] == \"read_file\":\n",
        "    result = {\"result\": read_file(action[\"args\"][\"file_name\"])}\n",
        "elif action[\"tool_name\"] == \"error\":\n",
        "    result = {\"error\": action[\"args\"][\"message\"]}\n",
        "elif action[\"tool_name\"] == \"terminate\":\n",
        "    print(action[\"args\"][\"message\"])\n",
        "    break\n",
        "else:\n",
        "    result = {\"error\":\"Unknown action: \"+action[\"tool_name\"]}"
      ],
      "metadata": {
        "id": "uaBgP9by3t1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, if the action specifies `tool_name` as `list_files` with empty `args`, the list_files() function is called, and the agent returns the list of files in the directory. Similarly, a `read_file` action extracts the filename from the arguments and retrieves its content.\n",
        "\n",
        "The execution step is the point where the agent performs tangible work, such as interacting with files or printing messages to the console. It bridges the decision-making process with concrete results that feed back into the agent’s memory for subsequent iterations."
      ],
      "metadata": {
        "id": "LfhUnH_s3urW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple AI Agent, Part 3"
      ],
      "metadata": {
        "id": "2Eft4H7T33bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Update the Agent’s Memory**\n",
        "\n",
        "After executing an action, the agent updates its memory with the results. Memory serves as the agent’s record of what has happened during the interaction, including user requests, the actions performed, and their outcomes. By appending this information to the memory, the agent retains context, enabling it to make more informed decisions in future iterations.\n",
        "\n",
        "In the code, memory is updated by extending it with both the LLM’s response (representing the agent’s intention) and the result of the executed action:"
      ],
      "metadata": {
        "id": "0Rq9VEl84fT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory.extend([\n",
        "    {\"role\": \"assistant\", \"content\": response},\n",
        "    {\"role\": \"user\", \"content\": json.dumps(result)}\n",
        "])"
      ],
      "metadata": {
        "id": "gkYXyd0S4wka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How This Works:**\n",
        "\n",
        "* The **assistant role** captures the structured response generated by the LLM.\n",
        "* The **user role** captures the feedback in the form of the action result, ensuring that the LLM has a clear understanding of what happened after the action was performed. The results of actions are always communicated back to the LLM with the “user” role.\n",
        "\n",
        "By keeping a running history of these exchanges, the agent maintains continuity, allowing it to refine its behavior dynamically as the memory grows and track the status of its work.\n",
        "\n"
      ],
      "metadata": {
        "id": "OZfVt_1n4w3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Decide Whether to Continue**\n",
        "\n",
        "The final step in each iteration of the agent loop is determining whether to continue or terminate. This decision is based on the action executed and the state of the task at hand. If the parsed action specifies `terminate`, or if a predefined condition (e.g., maximum iterations) is met, the agent ends its loop.\n",
        "\n",
        "In the code, this is implemented as a simple conditional check:"
      ],
      "metadata": {
        "id": "Ud0p8Zkp45K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if action[\"tool_name\"] == \"terminate\":\n",
        "    print(action[\"args\"][\"message\"])\n",
        "    break"
      ],
      "metadata": {
        "id": "VKuylNrA49YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the action specifies a termination, the loop exits, and the agent provides a closing message defined in the `terminate` action’s arguments. If no termination is triggered, the agent loops back to process the next user request or continue its task."
      ],
      "metadata": {
        "id": "51auz-3C4-kb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Iterative Adaptation**\n",
        "\n",
        "Imagine the agent is tasked with reading a file but encounters a missing filename in the initial request.\n",
        "\n",
        "1. In the first iteration, it executes `list_files` to retrieve the available files.\n",
        "2. Based on the memory of this result, it refines its next action, prompting the user to select a specific file.\n",
        "3. This iterative process continues until the task is completed or the agent determines that no further actions are required.\n",
        "\n",
        "Each loop iteration, the agent can look back at its memory to decide if it has completed the overall task. The memory is a critical part of deciding if the agent should continue or terminate. By deciding whether to continue at each step, the agent balances its ability to dynamically adapt to new information with the need to eventually conclude its task. The agent can also be instructed on when to terminate the loop, such as if more than two errors are encountered or if a specific condition is met."
      ],
      "metadata": {
        "id": "KcDx5DNH5ETp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NOO5ziTUK60k",
        "_7zHFObSKj79",
        "eBblgbEOL2xi",
        "tbXDOf4QMvll",
        "Ob0hu6x3Q_0M",
        "PBc8W71ThkBb",
        "DJsAgxjj1HS_",
        "2Eft4H7T33bb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}